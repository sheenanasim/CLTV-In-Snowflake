{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "865ca654-b38c-4b48-8b28-150714ddd233",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "# Train Sentiment Model\n\nThis notebook trains a sentiment prediction model.\n\n## Notebook Summary\n\n**It takes as inputs...**\n\n2. A range of standard packages for modelling and evaluation\n3. A custom py script for preparing the data\n4. A subset of survey responses that has been labelled for training \n\n**It applies the following process...**\n\n1. Data preperation\n2. Model evaluation\n3. Model selection (confirmed by the user)\n4. Export model to be used for sentiment prediction\n\n**It's outputs are...**\n\n1. Multinomial Naive Bayes sentiment prediction model\n\n\n## Further Notes\n**During the project multiple models were evaluated. It was found that:**\n\n1. Naive Bayes models were the quickest to train and performed well relative to other options.\n1. Of the Naive Bayes models, the multinomial varient performed best despite the class imbalance\n2. Extensive text preprocessing, as is done for topic modelling, degraded performance.\n3. A small range of hyperparameters consistently returned the best performance\n\n\n**Future Improvements:**\n\n1. Spell check on the text inputs to improve the BOW representation\n2. Large scale SVM model\n3. Ensemble of methods combining lexicon approaches with this custom model"
  },
  {
   "cell_type": "markdown",
   "id": "c1063371-e21c-44b1-ae26-1726cf3cdb0f",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "# 1 Package Imports and Constants\n\n## 1.0 Package Imports\n\nSnowflake offers an ml library that acts as a wrapper for the sci0kit learn library with distributed compute benefits. However, as of June 2024 that library, snowflake.ml, does not support all features critical to this project. Therefore, sklearn is still used. \n\nThe missing features are:\n- utils for resampling\n- CountVectorizer\n- BalancedAccuracyScore metric (although this could be derived directly)\n- ClassificationReport metric (although this could be derived directly)\n\nNB: If a wider grid search is desired, and this results in a longer than desired run time, we could use the snowflake.ml.modeling grid search for that step. Alternatively, use a snowpark optimized warehouse https://docs.snowflake.com/en/user-guide/warehouses-snowpark-optimized\nTODO: review the above content once the notebook is complete. "
  },
  {
   "cell_type": "code",
   "id": "5b001486-89d9-48a1-be11-8ba62811d34c",
   "metadata": {
    "language": "python",
    "name": "cell6",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark.context import get_active_session\n\n# Snowpark ML\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml._internal.utils import identifier\n#from snowflake.ml.modeling.model_selection import GridSearchCV # TODO: reinstate of the termined worker error is resolved\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport ast\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.utils import resample\n\n# models\nfrom sklearn.naive_bayes import MultinomialNB\n\n# evaluation\nfrom sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold, train_test_split, GridSearchCV\nfrom sklearn.metrics import balanced_accuracy_score , confusion_matrix",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "51b18306-f180-472f-89c2-70cd1483f146",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "## 1.1 Notebook Constants"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1",
    "codeCollapsed": false
   },
   "source": "# set seed for reproducible results\nseed = 1234\nnp.random.seed(seed)\nrandom_state = seed # for most actions\nrandom_state2 = seed-1 # to ensure different, but reproducible, inner folds in nested cv\n\n# nested cv config. Use stratified for imbalance\nouter_cv = RepeatedStratifiedKFold(n_splits=7, n_repeats=3, random_state=random_state)\ninner_cv = RepeatedStratifiedKFold(n_splits=7, n_repeats=3, random_state=random_state2) # random state2 so inner folds differ to outer folds\n\n# get session\nsession = get_active_session()\n\n# for debugging notebook\ndebug = False",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "38f8bbee-667d-4d24-a761-e234fdc667fb",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": "## 1.2 Import Data"
  },
  {
   "cell_type": "markdown",
   "id": "1a19232d-18ed-473b-a85a-cc0fec28d7c6",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "**Import pre-labeled data for training**"
  },
  {
   "cell_type": "code",
   "id": "b44e6148-b727-4bc9-9858-40654cd61eaa",
   "metadata": {
    "language": "python",
    "name": "cell8",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# NB: These labels could be pushed to a table and streamlit put over the top to enable more manual labelling\nlabelled_df = pd.read_csv('schl_aa_master_sentiment_labels.csv', encoding='utf-8', index_col=False)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3e451850-9e3f-4cf3-b671-55434007e888",
   "metadata": {
    "language": "python",
    "name": "cell9",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# demonstrate imbalance\nlabelled_df['sentiment'].value_counts()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fbecfe3b-a61f-452e-af38-a90cf7ad80e2",
   "metadata": {
    "name": "cell10",
    "collapsed": false
   },
   "source": "# 2 Process\n\n## 2.0 Data Preperation"
  },
  {
   "cell_type": "code",
   "id": "532ef5ef-b274-4a0b-ac08-10f3e51105e0",
   "metadata": {
    "language": "python",
    "name": "cell11",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# take a prepared dataframe, seperates the features (X) from the target (y) and creates a 80/20 split stratified by target\ndef get_splits(dataset, seed, random=False):\n    if random:\n        random_state = None\n    else:\n        random_state = seed\n\n    X = dataset['comment_response']\n    \n    y = dataset['sentiment']   \n    \n    # stratify as the classes are imbalanced and we want that imbalance to be consistent across splits\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = random_state, stratify=y)\n\n    return X_train, X_test, y_train, y_test",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "003a77a7-f8b3-4e9c-97bb-1838d723f6ae",
   "metadata": {
    "name": "cell13",
    "collapsed": false
   },
   "source": "## 2.1 Model Preperation\n\n**Processing Pipelines**"
  },
  {
   "cell_type": "code",
   "id": "a25e99e3-7115-4b66-a63f-4c1c7997614c",
   "metadata": {
    "language": "python",
    "name": "cell12",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# model pipeline\nmnb_clf = Pipeline([\n    ('vect', CountVectorizer()),\n    ('clf', MultinomialNB()),\n])\n\n# parameter ranges. TODO: reinstate once the issue with a vectorizer pipeline + GridSearchCV is resolved\n\n#n_grams = [(1,3)] # (1,3) consistently the best\n#alphas = [0.005, 0.01, 0.015] # narrow range. Best model is actually at 0.0075 but that will change with new data.\n#min_dfs = [10,15,50] # remove words that appear in less than X reviews. 10-15 was returning the best results.\n#max_dfs = np.linspace(0.5, 0.8, 3) # remove words that appear in more than X% of reviews. Decrease the last number to reduce the fit time.\n\nvect_params = {\n    'vect__min_df': min_dfs,\n    'vect__max_df': max_dfs,\n    'vect__ngram_range': n_grams,\n    'clf__alpha': alphas,\n}",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "56581584-3792-4bb9-873e-a42b3cd89fae",
   "metadata": {
    "name": "cell15",
    "collapsed": false
   },
   "source": "**Attempt 1: snowflake ml GridSearchCV within the original sklearn GridSearchCV code.**"
  },
  {
   "cell_type": "code",
   "id": "f24add85-d069-4ebb-b750-1f22a11f63b3",
   "metadata": {
    "language": "python",
    "name": "cell14",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def get_model_results(X_train, X_test, y_train, y_test, pipe, params):\n\n    # format for snowflake ml grid search which expects train data and labels in a single dataframe vs sklearn fit which expects two arrays\n    train_df = pd.concat([X_train, y_train], axis=1)\n    \n    # hyperparameter tunning\n    ## NB it appears SF notebooks do not allow jobs to run in parallel. n_jobs was changed from -2 to None \n    #grid = GridSearchCV(pipe, params, cv=inner_cv, scoring='balanced_accuracy', n_jobs=1,verbose=1) # use this once the range of hyperparameters is refined by halvinggridsearch\n    #grid_fit = grid.fit(X_train, y_train)\n    \n    # try snowflake ml grid search as sklearn version triggering memory/thread failures. TODO: review for final version\n    grid = GridSearchCV(estimator = pipe,\n                        param_grid = params, \n                        cv = inner_cv, \n                        scoring = 'balanced_accuracy',\n                        input_cols = 'comment_response',\n                        label_cols = 'sentiment',\n                        output_cols = 'predicted_sentiment')\n\n    print('grid search created')\n    \n\n    #test_df = pd.concat([X_test, y_test], axis=1)\n    \n    grid_fit = grid.fit(train_df)\n    print('grid fit done')\n    \n    # results\n    best_model = grid_fit.best_estimator_\n    tunning_cv_score = grid_fit.best_score_\n    \n    # evaluate best estimator\n    eval_cv = cross_val_score(best_model, X_train, y_train, cv=outer_cv,n_jobs=-1,scoring='balanced_accuracy')\n    \n    # results\n    eval_cv_score = eval_cv.mean()\n    nested_cv_error = tunning_cv_score - eval_cv_score\n    \n    # evaluate on hold out\n    predicted = best_model.predict(X_test)\n    test_score = balanced_accuracy_score(y_test, predicted)\n    approx_error = test_score - eval_cv_score\n\n    return best_model, tunning_cv_score, eval_cv, eval_cv_score, nested_cv_error,test_score, approx_error",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f58bbaed-a857-43f0-9a6e-69a8e5fa4dbc",
   "metadata": {
    "language": "python",
    "name": "sk_ml_gridsearch",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "if debug:\n\n    experiment_name = ['tuned']\n    X_train, X_test, y_train, y_test = get_splits(dataset=labelled_df[['sentiment','comment_response']],seed=seed,upsample=False)\n    \n    best_model,tunning_cv_score,eval_cv,eval_cv_score,nested_cv_error,test_score,approx_error = get_model_results(X_train, X_test, y_train, y_test, mnb_clf, vect_params)\n    \n    print('Nested cross validation score: {}'.format(test_score))\n    print('Test score: {}'.format(test_score))\n    print('Approximation error: {}'.format(test_score)) # the difference between cross validation score and test score. Small means lower risk of over/under fitting ",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2fd92e38-0348-412c-a22c-973421ae8bed",
   "metadata": {
    "name": "cell21",
    "collapsed": false
   },
   "source": "**Attempt 2: snowflake ml GridSearchCV outside the custom multi-model training function.**"
  },
  {
   "cell_type": "code",
   "id": "9d914474-4631-45fd-acfe-738b67fe4743",
   "metadata": {
    "language": "python",
    "name": "sf_ml_gridsearch",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.ml.modeling.model_selection import GridSearchCV\nif debug:\n    X_train, X_test, y_train, y_test = get_splits(dataset=labelled_df[['sentiment','comment_response']],seed=seed)\n    \n    train_df = pd.concat([X_train, y_train], axis=1)\n    \n    # hyperparameter tunning\n    ## NB it appears SF notebooks do not allow jobs to run in parallel. n_jobs was changed from -2 to None \n    #grid = GridSearchCV(pipe, params, cv=inner_cv, scoring='balanced_accuracy', n_jobs=1,verbose=1) # use this once the range of hyperparameters is refined by halvinggridsearch\n    #grid_fit = grid.fit(X_train, y_train)\n    \n    # try snowflake ml grid search as sklearn version triggering memory/thread failures. TODO: review for final version\n    grid = GridSearchCV(estimator = mnb_clf,\n                        param_grid = vect_params, \n                        #cv = inner_cv, \n                        scoring = 'balanced_accuracy',\n                        input_cols = 'comment_response',\n                        label_cols = 'sentiment',\n                        output_cols = 'predicted_sentiment')\n\n    grid.fit(train_df)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "01136e44-2aec-4ea3-9bdd-069fa89734d4",
   "metadata": {
    "name": "cell22",
    "collapsed": false
   },
   "source": "**Attempt 3: drop the GridSearchCV and hardcode the pipeline hyperparameters**\n\nThis successfully fits the pipeline (but relies on the user knowing the optimal hyperparameters)"
  },
  {
   "cell_type": "code",
   "id": "e4d174a7-adb3-4925-bf71-4798ae87b904",
   "metadata": {
    "language": "python",
    "name": "cell16",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# abandon the grid search and test if the model registry works for this pipeline. \n# start by fitting the pipeline with hard coded values\n\nX_train, X_test, y_train, y_test = get_splits(dataset=labelled_df[['sentiment','comment_response']],seed=seed)\n\n# model pipeline\nmnb_clf = Pipeline([\n    ('vect', CountVectorizer(ngram_range=(1,3),min_df=10,max_df=0.75)),\n    ('clf', MultinomialNB(alpha=0.0075)),\n])\n\nmnb_clf.fit(X_train, y_train)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5664fe12-c983-4925-9f57-39d87077e5c1",
   "metadata": {
    "language": "python",
    "name": "cell17",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "accuracy_score = mnb_clf.score(X_test, y_test)\nprint('Accuracy score: {}'.format(accuracy_score))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d851c62b-218f-481e-b8b0-faad96aa400d",
   "metadata": {
    "language": "python",
    "name": "cell24",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "y_pred = mnb_clf.predict(X_test)\nbal_accuracy_score = balanced_accuracy_score(y_test, y_pred)\nprint('Balanced accuracy score: {}'.format(bal_accuracy_score))",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ebeb51df-e208-447f-be87-10121f9468a1",
   "metadata": {
    "name": "save_model",
    "collapsed": false
   },
   "source": "## Save Model to Snowflake Model Registry"
  },
  {
   "cell_type": "code",
   "id": "c1b0839f-cadc-4020-b9fb-2edbeb9041aa",
   "metadata": {
    "language": "python",
    "name": "cell19"
   },
   "outputs": [],
   "source": "# TODO: update with the final location of the registry\n\ndb = identifier._get_unescaped_name(session.get_current_database())\nschema = 'ML_MODEL_REGISTRY'\n\nreg = Registry(session=session, database_name=db, schema_name=schema)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fb1979bf-a910-470f-839a-507f2110a154",
   "metadata": {
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": "# function to find the current model version and return the next version to use\ndef get_next_model_version(reg, model_name):\n    models = reg.show_models()\n    if models.empty:\n        return \"V_1\"\n    elif model_name not in models[\"name\"].to_list():\n        return \"V_1\"\n    max_version = max(\n        ast.literal_eval(models.loc[models[\"name\"] == model_name, \"versions\"].values[0])\n    )\n    return 'V_{}'.format({int(max_version.split('_')[-1]) + 1})",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d9ea1bfc-8d3a-4777-ac25-220336275f6b",
   "metadata": {
    "language": "python",
    "name": "cell20"
   },
   "outputs": [],
   "source": "# Log the model\n# the name + version must be unique and both are strings. The name could be a UUID but a short description of the use case is more practical.\n# the model can be logged with metrics, to track history, and the features\n\nmodel_name = \"CEMPLICITY_SENTIMENT_CLASSIFIER\"\n\nlogged_model = reg.log_model(\n        model_name = model_name,\n        version_name = get_next_model_version(reg, model_name), # assumes versioning is incremental\n        model = mnb_clf,\n        metrics = {'accuracy': accuracy_score,\n                   'balanced_accuracy': bal_accuracy_score},\n        sample_input_data = pd.DataFrame(X_train[:100]))\n\n# get around bug with log_model method not saving comments\nlogged_model.comment = 'Test model registry for hosting. This is the sentiment classifier trained on the Cemplicity survey data.'",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "44722517-543b-425c-884d-5d243f39c85f",
   "metadata": {
    "language": "python",
    "name": "cell23",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# check the model has been saved\nreg.get_model(model_name).show_versions()",
   "execution_count": null
  }
 ]
}